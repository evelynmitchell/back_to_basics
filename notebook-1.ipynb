{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - torchtext\n",
    "# (https://pytorch.org/text/stable/index.html)\n",
    "\n",
    "This library is used for text processing in Pytorch.\n",
    "\n",
    "It includes neural network primitives (nn), utility functions for data handling (functional), metrics, tokenizers(utils), datasets, vocabulary manipulation(vocab), utlities for file handling (utils),\n",
    "transforms, data transform functions (functional), and models.\n",
    "\n",
    "The datasets are in torchtext.datasets, and are task specific for Text Classification, Language Modeling, Machine Translation, Sequence Tagging, Question Answer, Unsupervised Learning.\n",
    "\n",
    "The transforms are common text transforms, such as tokenization, image to text and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network primitives (nn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Metrics\n",
    "\n",
    "### BLEU score\n",
    "\n",
    "For generated text, a useful score is the precision[1] of the generated output. The number of words in the generated text that appear in the reference text. This is called the BLEU score, for Bilingual Evaluation Understudy. The comparison is done over the length of the reference text, and the vanilla version of BLEU does not adjust for the length of the reference text. So if you generate a single word that appears in a long work, and repeat it many times, you will have a good BLEU score, because the word appears in the reference text, but it isn't meaningful, because a single word repeated is not a good representation of the reference text.\n",
    "\n",
    "BLEU is a useful metric for machine translation, where the reference text is a translation of the generated text. It is less useful for other applications, where the reference text is not a translation of the generated text.\n",
    "\n",
    "The BLEU score is adjusted for the frequency of occurrance of each word in the reference text. So, a good translation of the reference text will match the word frequency as well as the length of the reference text.    `\n",
    "\n",
    "See also [F1 score](https://en.wikipedia.org/wiki/F1_score) which is the harmonic mean of precision and recall.\n",
    "\n",
    "### References\n",
    "\n",
    "[1](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "[2](https://en.wikipedia.org/wiki/F-score)\n",
    "[BLEU score](https://pytorch.org/text/stable/data_metrics.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Before we can pass text into a model for training or inference, we need to clean it up, and put it in a form suitable for processing. For example, we may want to remove punctuation, split the text into words on whitespace, and lowercase the words. In addition, we may want to further split the words into subwords, or small groups of characters.\n",
    "\n",
    "There are many different tokenizers, and if you are using a pre-trained model, you will need to use the same tokenizer the model was trained with. \n",
    "\n",
    "Pytorch and therefore zetascale, includes several different tokenizers. The most common is the [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokenizer, which is used by the [GPT-2](https://openai.com/blog/better-language-models/) and [GPT-3](https://arxiv.org/abs/2005.14165) models.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
